\subsection{Numerical Methods}
\label{sec:numerical-methods}
Theorem \ref{thm:main} and the closed-form solutions yield efficient algorithms for solving the cost minimization problem.

Consider a relaxed cost minimization problem where we also include a finite number of global incentive compatibility constraints at a vector of actions $\boldsymbol{\hat a} = (\hat a_1, \ldots, \hat a_n)$. The Lagrangian from equation (\ref{eq:lagrangian}) becomes
\[
    \begin{aligned}
    \mathcal{L}(v,\lambda,\mu, \boldsymbol{\hat \mu}, \boldsymbol{\hat a})
    &:=
    W(v, a_0)
    +\lambda\left(\bar{U}-U(v, a_0)\right)
    +\mu(-U_{a}(v, a_0))\\
    &\quad+\sum \hat \mu_i \left(U(v, \hat a_i) - U(v, a_0)\right)
    \text{.}
    \end{aligned}
\]

Heuristically differentiating pointwise with respect to $v(y)$ and setting the derivative to zero gives
\[
    k'(v(y)) f(y | a_0)
    =
    \lambda f(y | a_0)
    + \mu f_{a}(y|a_0)
    + \sum \hat \mu_i [f(y | a_0) - f(y | \hat a_i)]
    \text{.}
\]

The solution $v$ equals
\[
    V(y | \lambda, \mu, \boldsymbol{\hat \mu}, \boldsymbol{\hat a})
    :=
    g\biggl(
        \lambda + \mu S(y|a_0) + \sum \hat \mu_i \left(1 - \frac{f(y | \hat a_i)}{f(y | a_0)}\right)
    \biggr)
    \text{.}\
\]

Define the Lagrange dual function as
\[
    \mathcal{D}(\lambda, \mu, \boldsymbol{\hat \mu}, \boldsymbol{\hat a})
    := \sup_{v} 
    \mathcal{L}(v, \lambda, \mu, \boldsymbol{\hat \mu}, \boldsymbol{\hat a})
    =
    \mathcal{L}(V(y | \lambda, \mu, \boldsymbol{\hat a}), \lambda, \mu, \boldsymbol{\hat a})
    \text{.}
\]

For any parametric example, the dual can be computed efficiently with the analytical formulas from section \ref{sec:calculus}. It also has an analytic gradient by Danskin’s envelope theorem. Moreover, given a grid of outputs $\mathbf{y}_{\mathrm{grid}}$, we can cache $f(y | a_0)$, $S(y | a_0)$, and $1 - {f(y | \hat a_i)}/{f(y | a_0)}$. This is enough to perform the numerical integrations needed for $\mathcal D$, so that $\mathcal D$ only involves matrix multiplications and applications of $g$ and $k$.

This suggests the following algorithm \ref{alg:cost-minimization}:

\begin{algorithm}[H]
    \DontPrintSemicolon % Removes semicolons at end of lines for cleaner look
    \SetAlgoLined
    
    % Define keywords for functions to make them bold/distinct
    \SetKwFunction{FindBest}{FindBestDeviation}
    \SetKwFunction{MaximizeDual}{MaximizeDual}
    \SetKwFunction{InitGrid}{InitializeGrid}
    \SetKwFunction{UpdateCache}{UpdateCache}

    % Initialize variables
    % Using \tcp for comments ensures proper alignment
    $\boldsymbol{\hat a} \gets \emptyset$; \quad
    $\boldsymbol{\hat\mu} \gets \emptyset$; \quad
    $\lambda \gets \lambda_{\text{init}}$; \quad
    $\mu \gets \mu_{\text{init}}$\;
    
    $\mathbf{y}_{\mathrm{grid}} \gets \InitGrid()$\;

    \BlankLine
    \Repeat{$\mathtt{deviation\_gain} \le \mathtt{tolerance}$}{
        \UpdateCache{$\mathbf{y}_{\mathrm{grid}},\, a_0,\, \boldsymbol{\hat a}$} \tcp*[r]{For fast dual calls}
        
        $(\lambda,\, \mu,\, \boldsymbol{\hat\mu})
        \gets
        \MaximizeDual{$\mathcal{D}, \text{init}=\{\lambda, \mu, \boldsymbol{\hat\mu}\}$}$
        \tcp*[r]{use warm start}


        $v(\mathbf{y}_{\mathrm{grid}}) \gets V(\mathbf{y}_{\mathrm{grid}} \mid \lambda, \mu, \boldsymbol{\hat\mu}, \boldsymbol{\hat a})$\;

        $\mathtt{best\_deviation},\;\mathtt{deviation\_gain}
        \gets
        \FindBest{$v,\, a_0$}$\;

        \If{$\mathtt{deviation\_gain} > \mathtt{tolerance}$}{
            $\boldsymbol{\hat a} \gets \boldsymbol{\hat a} \cup \{\mathtt{best\_deviation}\}$\;
            $\boldsymbol{\hat\mu} \gets (\boldsymbol{\hat\mu}, 0)$ \tcp*[r]{Warm start update}
        }
    }

    \Return{$v$}

    \caption{Cost minimization via active–set}
    \label{alg:cost-minimization}
\end{algorithm}

Algorithm \ref{alg:cost-minimization} solves all the examples in the paper. In our benchmarks, each run takes in the order of 1.5 to 8 milliseconds (see supplementary material). Performance seems to be due to three factors. First, the algorithm often ends in the first iteration because of Theorem \ref{thm:main}. Second, dual calls are fast because of the analytic formulas and caching. Third, dual maximization is fast because of low dimensionality, analytic gradients and warm starts.

Algorithm \ref{alg:cost-minimization} has two main drawbacks. The dual is not convex, so that the optimization step can get stuck in a local maximum. And the heuristic to add constraints might also get stuck.

To overcome these drawbacks, we implement a straightforward convex optimization solver. We discretize both the outcome and action spaces, which yields a problem with a convex objective and linear constraints. We solve this discretized problem using \texttt{CVXPY} with the \texttt{Clarabel} solver. This approach is computationally more expensive—taking between 140 and 230 milliseconds in our benchmarks—because it solves for the contract $v$ at every point of the outcome grid. However, it provides a global optimality guarantee. In our implementation, Algorithm~\ref{alg:cost-minimization} uses this convex solver as a fallback.

The main drawback of the convex solver is that the numerical optimal contract $v(y)$ is numerically unstable at points where the density $f(y | a_0)$ is small. This is illustrated in Figure \ref{fig:convex-solver-stability}. The figure shows that both solvers reach essentially the same objective value. In the middle of the distribution, they reach essentially the same optimal contract. However, in the tails of the distribution, the convex solver optimal contract is numerically unstable.

\begin{figure}[p]
    \centering
    \includegraphics[width=\textwidth]{figures/solver_comparison/stacked_comparison.pdf}
    \captionsetup{font=footnotesize}
    \caption{Comparison of Algorithm 1 and convex program solvers.}
    \label{fig:convex-solver-stability}
    \caption*{\textit{Note:} Top panel: optimal wage function $w(y)$. Bottom panel: agent's expected utility $U(v^*, a)$. Both solvers reach essentially the same expected wage cost. The convex program solution is numerically unstable in the tails where the density $f(y | a_0)$ is small. Parameters are as in Figure \ref{fig:gaussian-log-pp} with $\sigma = 10$. The convex solver uses a grid of 201 points for the outcomes and 200 points for the actions.}
\end{figure}

